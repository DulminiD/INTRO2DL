{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:55:51.161763Z",
          "iopub.status.busy": "2022-01-22T15:55:51.161511Z",
          "iopub.status.idle": "2022-01-22T15:55:51.165623Z",
          "shell.execute_reply": "2022-01-22T15:55:51.164626Z",
          "shell.execute_reply.started": "2022-01-22T15:55:51.161734Z"
        },
        "trusted": true,
        "id": "Jc1HacqoP0dP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import spacy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glvPwcm0P0dS"
      },
      "source": [
        "# Import Required Libraries & Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:55:51.178773Z",
          "iopub.status.busy": "2022-01-22T15:55:51.178323Z",
          "iopub.status.idle": "2022-01-22T15:55:52.913605Z",
          "shell.execute_reply": "2022-01-22T15:55:52.912893Z",
          "shell.execute_reply.started": "2022-01-22T15:55:51.178736Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "zW9I8GNXP0dU",
        "outputId": "c3a7b4b2-7ba9-4d8b-801d-f9d0434cfcd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "5  Probably my all-time favorite movie, a story o...  positive\n",
              "6  I sure would like to see a resurrection of a u...  positive\n",
              "7  This show was an amazing, fresh & innovative i...  negative\n",
              "8  Encouraged by the positive comments about this...  negative\n",
              "9  If you like original gut wrenching laughter yo...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c2ea1da-1175-40b9-a4ab-3785516f0b11\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my all-time favorite movie, a story o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a u...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter yo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c2ea1da-1175-40b9-a4ab-3785516f0b11')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3c2ea1da-1175-40b9-a4ab-3785516f0b11 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3c2ea1da-1175-40b9-a4ab-3785516f0b11');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#importing the training data\n",
        "df=pd.read_csv('IMDB Dataset.csv')\n",
        "print(df.shape)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_JCywfAP0dW"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:55:52.915240Z",
          "iopub.status.busy": "2022-01-22T15:55:52.914817Z",
          "iopub.status.idle": "2022-01-22T15:55:52.931074Z",
          "shell.execute_reply": "2022-01-22T15:55:52.930198Z",
          "shell.execute_reply.started": "2022-01-22T15:55:52.915202Z"
        },
        "trusted": true,
        "id": "SqZxRYWwP0dW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "sentiment : 0 = negative, 1 = positive \n",
        "use the following to get the sentiment of a sentence :  \n",
        "sentiment = 0 if sentiment is negative else 1\n",
        "\n",
        "\n",
        "use np.where to get the sentiment of a sentence :\n",
        "\"\"\"\n",
        "df['sentiment'] = np.where(df['sentiment'] == 'positive', 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:55:52.933556Z",
          "iopub.status.busy": "2022-01-22T15:55:52.933295Z",
          "iopub.status.idle": "2022-01-22T15:55:52.942961Z",
          "shell.execute_reply": "2022-01-22T15:55:52.942162Z",
          "shell.execute_reply.started": "2022-01-22T15:55:52.933524Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XDoc_uj-P0dX",
        "outputId": "5c1a07ca-7735-4009-d79b-1d2076f7c652"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  One of the other reviewers has mentioned that ...          1\n",
              "1  A wonderful little production. <br /><br />The...          1\n",
              "2  I thought this was a wonderful way to spend ti...          1\n",
              "3  Basically there's a family where a little boy ...          0\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9a991e8-dff4-42af-8cdb-4a01b264314d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9a991e8-dff4-42af-8cdb-4a01b264314d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f9a991e8-dff4-42af-8cdb-4a01b264314d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f9a991e8-dff4-42af-8cdb-4a01b264314d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:55:52.944777Z",
          "iopub.status.busy": "2022-01-22T15:55:52.944484Z",
          "iopub.status.idle": "2022-01-22T15:55:52.951178Z",
          "shell.execute_reply": "2022-01-22T15:55:52.950461Z",
          "shell.execute_reply.started": "2022-01-22T15:55:52.944743Z"
        },
        "trusted": true,
        "id": "elKcgd9xP0dY"
      },
      "outputs": [],
      "source": [
        "df.columns = ['TEXT_COLUMN_NAME', 'LABEL_COLUMN_NAME']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['TEXT_COLUMN_NAME'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZpRvPekTgyt",
        "outputId": "150669f0-637c-476d-9234-b99cf628299d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        One of the other reviewers has mentioned that ...\n",
            "1        A wonderful little production. <br /><br />The...\n",
            "2        I thought this was a wonderful way to spend ti...\n",
            "3        Basically there's a family where a little boy ...\n",
            "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
            "                               ...                        \n",
            "49995    I thought this movie did a down right good job...\n",
            "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
            "49997    I am a Catholic taught in parochial elementary...\n",
            "49998    I'm going to have to disagree with the previou...\n",
            "49999    No one expects the Star Trek movies to be high...\n",
            "Name: TEXT_COLUMN_NAME, Length: 50000, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['totalwords'] = df['TEXT_COLUMN_NAME'].str.split().str.len()\n",
        "print(df['totalwords'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTCmPbRWbvrY",
        "outputId": "826e4ab1-9a9e-4423-9a54-c2bec296713b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        307\n",
            "1        162\n",
            "2        166\n",
            "3        138\n",
            "4        230\n",
            "        ... \n",
            "49995    194\n",
            "49996    112\n",
            "49997    230\n",
            "49998    212\n",
            "49999    129\n",
            "Name: totalwords, Length: 50000, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"Zero-length reviews: {}\".format(df['totalwords'].min()))\n",
        "print(\"Maximum review length: {}\".format(df['totalwords'].max()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdL2w63PcZ2m",
        "outputId": "09bdcf66-a23b-43fc-8a27-0886b60f07e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-length reviews: 4\n",
            "Maximum review length: 2470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def pad_features(reviews_ints, seq_length):\n",
        "#     ''' Return features of review_ints, where each review is padded with 0's \n",
        "#         or truncated to the input seq_length.\n",
        "#     '''\n",
        "#     ## getting the correct rows x cols shape\n",
        "#     features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "    \n",
        "#     ## for each review, I grab that review\n",
        "#     for i, row in enumerate(reviews_ints):\n",
        "#       features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "    \n",
        "#     return features"
      ],
      "metadata": {
        "id": "EhPejoK-dqaL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seq_length = 200\n",
        "\n",
        "# features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "\n",
        "# ## test statements - do not change - ##\n",
        "# assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
        "# assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
        "\n",
        "# # print first 10 values of the first 30 batches \n",
        "# print(features[:30,:10])"
      ],
      "metadata": {
        "id": "ayrwbGRxi_CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:55:52.953080Z",
          "iopub.status.busy": "2022-01-22T15:55:52.952741Z",
          "iopub.status.idle": "2022-01-22T15:55:53.553818Z",
          "shell.execute_reply": "2022-01-22T15:55:53.553120Z",
          "shell.execute_reply.started": "2022-01-22T15:55:52.952969Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLIT8nwPP0dY",
        "outputId": "1201b2e1-2288-48b5-bd18-489e9216514b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7fba6af1eb50>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "\"\"\"\n",
        "Load the spacy model and load the English language model from https://spacy.io/usage/models\n",
        "\"\"\"\n",
        "spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:55:53.629426Z",
          "iopub.status.busy": "2022-01-22T15:55:53.629143Z",
          "iopub.status.idle": "2022-01-22T15:55:53.637675Z",
          "shell.execute_reply": "2022-01-22T15:55:53.636930Z",
          "shell.execute_reply.started": "2022-01-22T15:55:53.629373Z"
        },
        "trusted": true,
        "id": "bwNEndVUP0dZ"
      },
      "outputs": [],
      "source": [
        "# general Settings\n",
        "\n",
        "RANDOM_SEED = 123\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "VOCABULARY_SIZE = 20000\n",
        "LEARNING_RATE = 0.01\n",
        "BATCH_SIZE = 50\n",
        "NUM_EPOCHS = 30\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM = 256\n",
        "NUM_CLASSES = 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch==1.8.0 torchtext==0.9.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "R9gFYUCIaNt1",
        "outputId": "7f65e46c-2c09-43cd-d101-d83e41c5831f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.8.0\n",
            "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.5 MB 14 kB/s \n",
            "\u001b[?25hCollecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 35.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (4.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0 torchtext-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7T1bAOuP0dZ"
      },
      "source": [
        "# Text & label Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:55:53.639530Z",
          "iopub.status.busy": "2022-01-22T15:55:53.639045Z",
          "iopub.status.idle": "2022-01-22T15:55:54.191068Z",
          "shell.execute_reply": "2022-01-22T15:55:54.190361Z",
          "shell.execute_reply.started": "2022-01-22T15:55:53.639494Z"
        },
        "trusted": true,
        "id": "JV8yExpfP0da"
      },
      "outputs": [],
      "source": [
        "# Define feature processing\n",
        "\"\"\"\n",
        "Define the fields for the data.\n",
        "\"\"\"\n",
        "TEXT = torchtext.legacy.data.Field(tokenize = 'spacy', tokenizer_language = 'en_core_web_sm', fix_length=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:55:54.194504Z",
          "iopub.status.busy": "2022-01-22T15:55:54.194231Z",
          "iopub.status.idle": "2022-01-22T15:55:54.198370Z",
          "shell.execute_reply": "2022-01-22T15:55:54.197471Z",
          "shell.execute_reply.started": "2022-01-22T15:55:54.194470Z"
        },
        "trusted": true,
        "id": "ikRAJFL6P0da"
      },
      "outputs": [],
      "source": [
        "# Define Label processing\n",
        "LABEL = torchtext.legacy.data.LabelField(dtype = torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:55:54.200276Z",
          "iopub.status.busy": "2022-01-22T15:55:54.199959Z",
          "iopub.status.idle": "2022-01-22T15:55:57.099915Z",
          "shell.execute_reply": "2022-01-22T15:55:57.099168Z",
          "shell.execute_reply.started": "2022-01-22T15:55:54.200243Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "wPI85A0YP0da",
        "outputId": "51345fe0-e6be-44d9-94ee-7330b80c8edd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    TEXT_COLUMN_NAME  LABEL_COLUMN_NAME  \\\n",
              "0  One of the other reviewers has mentioned that ...                  1   \n",
              "1  A wonderful little production. <br /><br />The...                  1   \n",
              "2  I thought this was a wonderful way to spend ti...                  1   \n",
              "3  Basically there's a family where a little boy ...                  0   \n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...                  1   \n",
              "\n",
              "   totalwords  \n",
              "0         307  \n",
              "1         162  \n",
              "2         166  \n",
              "3         138  \n",
              "4         230  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-78984184-e012-4d9f-8647-c168f25e80a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXT_COLUMN_NAME</th>\n",
              "      <th>LABEL_COLUMN_NAME</th>\n",
              "      <th>totalwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "      <td>307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "      <td>162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "      <td>166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "      <td>138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "      <td>230</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78984184-e012-4d9f-8647-c168f25e80a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-78984184-e012-4d9f-8647-c168f25e80a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-78984184-e012-4d9f-8647-c168f25e80a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\"\"\"\n",
        "Define the fields for the data.\n",
        "\"\"\"\n",
        "\n",
        "df.to_csv('moviedata.csv', index = None)\n",
        "df = pd.read_csv('moviedata.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:55:57.102513Z",
          "iopub.status.busy": "2022-01-22T15:55:57.102056Z",
          "iopub.status.idle": "2022-01-22T15:56:41.889075Z",
          "shell.execute_reply": "2022-01-22T15:56:41.888358Z",
          "shell.execute_reply.started": "2022-01-22T15:55:57.102470Z"
        },
        "trusted": true,
        "id": "0BPlfwRGP0db"
      },
      "outputs": [],
      "source": [
        "# process the dataset\n",
        "\n",
        "fields = [('TEXT_COLUMN_NAME', TEXT), ('LABEL_COLUMN_NAME', LABEL)]\n",
        "\n",
        "dataset = torchtext.legacy.data.TabularDataset(\n",
        "                    path = \"moviedata.csv\",\n",
        "                    format = \"CSV\",\n",
        "                    skip_header = True,\n",
        "                    fields =  [('TEXT_COLUMN_NAME', TEXT),('LABEL_COLUMN_NAME',LABEL)]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgqD7uzyP0db"
      },
      "source": [
        "# Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:56:41.890714Z",
          "iopub.status.busy": "2022-01-22T15:56:41.890459Z",
          "iopub.status.idle": "2022-01-22T15:56:41.961168Z",
          "shell.execute_reply": "2022-01-22T15:56:41.960371Z",
          "shell.execute_reply.started": "2022-01-22T15:56:41.890680Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtW1NtF_P0db",
        "outputId": "efdfaff8-02f8-4486-bd90-d4201e42f7cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train data 40000\n",
            "Length of test data 10000\n"
          ]
        }
      ],
      "source": [
        "# Split dataset into train and test set\n",
        "\n",
        "train_data, test_data = dataset.split(split_ratio = [0.8, 0.2], random_state = random.seed(RANDOM_SEED))\n",
        "\n",
        "print('Length of train data', len(train_data))\n",
        "print('Length of test data', len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:56:41.962867Z",
          "iopub.status.busy": "2022-01-22T15:56:41.962611Z",
          "iopub.status.idle": "2022-01-22T15:56:42.018970Z",
          "shell.execute_reply": "2022-01-22T15:56:42.017378Z",
          "shell.execute_reply.started": "2022-01-22T15:56:41.962832Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krzzgiyAP0dc",
        "outputId": "c20a56a3-188b-4892-c2f9-8a0cd77ba551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train data 34000\n",
            "Length of valid data 6000\n"
          ]
        }
      ],
      "source": [
        "train_data, val_data = train_data.split(split_ratio = [0.85, 0.15], random_state = random.seed(RANDOM_SEED))\n",
        "\n",
        "print('Length of train data', len(train_data))\n",
        "print('Length of valid data', len(val_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd35pDD2Xghi",
        "outputId": "763de44a-1f66-4d7d-8113-8b8b75d7e806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torchtext.legacy.data.example.Example object at 0x7f9785ce9790>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOQ50G-7P0dc"
      },
      "source": [
        "# Data Observation after Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:56:42.020384Z",
          "iopub.status.busy": "2022-01-22T15:56:42.020132Z",
          "iopub.status.idle": "2022-01-22T15:56:42.026208Z",
          "shell.execute_reply": "2022-01-22T15:56:42.025377Z",
          "shell.execute_reply.started": "2022-01-22T15:56:42.020349Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c5ALmeOP0dd",
        "outputId": "1e05934a-9f88-4432-e342-063c123a66ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'TEXT_COLUMN_NAME': ['Flipping', 'through', 'the', 'channels', 'I', 'was', 'lucky', 'enough', 'to', 'stumble', 'upon', 'the', 'beginning', 'of', 'this', 'movie', '.', 'I', 'must', 'admit', 'that', 'it', 'grabbed', 'my', 'attention', 'almost', 'immediately', '.', 'I', 'love', 'older', 'films', 'and', 'this', 'is', 'or', 'should', 'be', 'considered', 'a', 'classic', '!', 'One', 'of', 'the', 'most', 'wonderful', 'rarities', 'of', 'this', 'movie', 'is', 'that', 'the', 'main', 'character', 'was', 'not', 'only', 'female', 'but', 'she', 'was', 'also', 'a', 'bad', 'girl', '.', 'I', 'highly', 'recommend', 'this', 'movie', '!'], 'LABEL_COLUMN_NAME': '1'}\n"
          ]
        }
      ],
      "source": [
        "# Look at first traning example\n",
        "\n",
        "print(vars(train_data.examples[2000]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:56:42.028212Z",
          "iopub.status.busy": "2022-01-22T15:56:42.027904Z",
          "iopub.status.idle": "2022-01-22T15:56:43.688300Z",
          "shell.execute_reply": "2022-01-22T15:56:43.687534Z",
          "shell.execute_reply.started": "2022-01-22T15:56:42.028178Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWzGKnhwP0dd",
        "outputId": "fc9121f1-d79c-475a-855b-bb3b8ec3290f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary size: 20002\n",
            "Label Size: 2\n",
            "<torchtext.vocab.Vocab object at 0x7fba956ebed0>\n"
          ]
        }
      ],
      "source": [
        "from typing import Text\n",
        "# Build Vocabulary\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = VOCABULARY_SIZE)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f'vocabulary size: {len(TEXT.vocab)}')\n",
        "print(f'Label Size: {len(LABEL.vocab)}')\n",
        "\n",
        "print(TEXT.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JnsrHJEfCM9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKrYHX1aP0de"
      },
      "source": [
        " 2 extra value in vocabulary is because added (unknown) and (padding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:56:43.690179Z",
          "iopub.status.busy": "2022-01-22T15:56:43.689727Z",
          "iopub.status.idle": "2022-01-22T15:56:43.726493Z",
          "shell.execute_reply": "2022-01-22T15:56:43.725793Z",
          "shell.execute_reply.started": "2022-01-22T15:56:43.690127Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RRt7Xf5P0de",
        "outputId": "edb7a8d7-7e62-4d5a-e37a-a5ff2cd4a2c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 390972), (',', 369444), ('.', 318719), ('a', 210502), ('and', 210006), ('of', 194658), ('to', 180163), ('is', 145895), ('in', 118266), ('I', 105681), ('it', 103588), ('that', 93995), ('\"', 85530), (\"'s\", 83147), ('this', 81771), ('-', 70411), ('/><br', 68787), ('was', 67372), ('as', 57734), ('movie', 57571), ('with', 57543), ('for', 56557), ('film', 52382), ('The', 51249), ('but', 46574), ('(', 44823), (')', 44506), ('on', 44484), (\"n't\", 44474), ('you', 42196)]\n"
          ]
        }
      ],
      "source": [
        "# Print the most common words: Use the most_common method of the TEXT vocabulary\n",
        "\n",
        "most_common_words = TEXT.vocab.freqs.most_common(30)\n",
        "print(most_common_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:56:43.727759Z",
          "iopub.status.busy": "2022-01-22T15:56:43.727533Z",
          "iopub.status.idle": "2022-01-22T15:56:43.733148Z",
          "shell.execute_reply": "2022-01-22T15:56:43.732231Z",
          "shell.execute_reply.started": "2022-01-22T15:56:43.727727Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jby-idTQP0df",
        "outputId": "f37b10e6-807d-46d8-8138-6104643d269b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is', 'in', 'I', 'it', 'that', '\"', \"'s\", 'this', '-', '/><br', 'was']\n"
          ]
        }
      ],
      "source": [
        "# Token corresponding to first 10 Indices\n",
        "\n",
        "print(TEXT.vocab.itos[:20]) #itos = Integer to string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from string import punctuation\n",
        "\n",
        "# all_text = ''.join([c for c in str(TEXT.vocab.itos).replace(',', ' ') if c not in punctuation])\n",
        "\n",
        "# reviews_ints = []\n",
        "# for review in all_text:\n",
        "#   reviews_ints.append([TEXT.vocab[word] for word in review.split()])\n",
        "\n",
        "# # print tokens in first review\n",
        "# print('Tokenized review: \\n', reviews_ints[:1])"
      ],
      "metadata": {
        "id": "zdevyxwaUVV1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gwAHLUJP0dg"
      },
      "source": [
        "# Data Preparation for Batch wise Implimentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:56:43.760843Z",
          "iopub.status.busy": "2022-01-22T15:56:43.760082Z",
          "iopub.status.idle": "2022-01-22T15:56:43.766731Z",
          "shell.execute_reply": "2022-01-22T15:56:43.766017Z",
          "shell.execute_reply.started": "2022-01-22T15:56:43.760804Z"
        },
        "trusted": true,
        "id": "WNcPpPEFP0dg"
      },
      "outputs": [],
      "source": [
        "# Define Dataloader\n",
        "\n",
        "train_loader, valid_loader, test_loader = torchtext.legacy.data.BucketIterator.splits(\n",
        "        (train_data, val_data, test_data), ### ADD YOUR SPLIT DATA HERE (Make sure you add it in a tuple) ###\n",
        "        batch_sizes = (50,50,50), ### ADD YOUR BATCH SIZE HERE ###\n",
        "        sort_within_batch = False, ### ADD YOUR SORT WITHIN BATCH HERE ### \n",
        "        sort_key = lambda x : len(x.TEXT_COLUMN_NAME), \n",
        "        device = DEVICE\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:56:43.768827Z",
          "iopub.status.busy": "2022-01-22T15:56:43.768197Z",
          "iopub.status.idle": "2022-01-22T15:56:43.900963Z",
          "shell.execute_reply": "2022-01-22T15:56:43.900283Z",
          "shell.execute_reply.started": "2022-01-22T15:56:43.768790Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CTeVVkEP0dg",
        "outputId": "6e1592c7-7fe3-40b1-efc3-6fec3c7989ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            "Text matrix size: torch.Size([256, 50])\n",
            "Target vector size: torch.Size([50])\n",
            "\n",
            "Valid:\n",
            "Text matrix size: torch.Size([256, 50])\n",
            "Target vector size: torch.Size([50])\n",
            "\n",
            "Test:\n",
            "Text matrix size: torch.Size([256, 50])\n",
            "Target vector size: torch.Size([50])\n"
          ]
        }
      ],
      "source": [
        "# Testing the iterators (note that the number of rows depends on the longest document in the respective batch):\n",
        "\n",
        "print('Train')\n",
        "for batch in train_loader:\n",
        "    print(f'Text matrix size: {batch.TEXT_COLUMN_NAME.size()}')\n",
        "    print(f'Target vector size: {batch.LABEL_COLUMN_NAME.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nValid:')\n",
        "for batch in valid_loader:\n",
        "    print(f'Text matrix size: {batch.TEXT_COLUMN_NAME.size()}')\n",
        "    print(f'Target vector size: {batch.LABEL_COLUMN_NAME.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nTest:')\n",
        "for batch in test_loader:\n",
        "    print(f'Text matrix size: {batch.TEXT_COLUMN_NAME.size()}')\n",
        "    print(f'Target vector size: {batch.LABEL_COLUMN_NAME.size()}')\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:56:43.902778Z",
          "iopub.status.busy": "2022-01-22T15:56:43.902325Z",
          "iopub.status.idle": "2022-01-22T15:56:43.909421Z",
          "shell.execute_reply": "2022-01-22T15:56:43.908776Z",
          "shell.execute_reply.started": "2022-01-22T15:56:43.902740Z"
        },
        "trusted": true,
        "id": "8QOXCO75P0dh"
      },
      "outputs": [],
      "source": [
        "# train_loader, valid_loader, test_loader = torchtext.legacy.data.BucketIterator.splits(\n",
        "#         [train_data, val_data, test_data], ### ADD YOUR SPLIT DATA HERE (Make sure you add it in a tuple) ###\n",
        "#         batch_size = 5, ### ADD YOUR BATCH SIZE HERE ###\n",
        "#         sort_within_batch = 5, ### ADD YOUR SORT WITHIN BATCH HERE ### \n",
        "#         sort_key = lambda x : len(x.TEXT_COLUMN_NAME), \n",
        "#         device = DEVICE\n",
        "#     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HO1lHaGP0dh"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:56:43.910544Z",
          "iopub.status.busy": "2022-01-22T15:56:43.910274Z",
          "iopub.status.idle": "2022-01-22T15:56:43.920687Z",
          "shell.execute_reply": "2022-01-22T15:56:43.919977Z",
          "shell.execute_reply.started": "2022-01-22T15:56:43.910510Z"
        },
        "trusted": true,
        "id": "nf_h_v5qP0dh"
      },
      "outputs": [],
      "source": [
        "class RNN(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, drop_prob=0.5):\n",
        "        super().__init__()\n",
        "        ### ADD YOUR CODE HERE ###\n",
        "        print('Model')\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
        "        \n",
        "        #lstm\n",
        "        self.lstm = torch.nn.LSTM(input_size=embedding_dim,hidden_size=hidden_dim)\n",
        "    \n",
        "        # linear and sigmoid layer\n",
        "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
        "        self.sig = torch.nn.Sigmoid()\n",
        "        \n",
        "        ### END YOUR CODE ### \n",
        "\n",
        "    def forward(self, text):\n",
        "        ### ADD YOUR CODE HERE ###\n",
        "        \n",
        "        batch_size = text.size(0)\n",
        "        \n",
        "        embeds = self.embedding(text) \n",
        "\n",
        "        lstm_out, hidden = self.lstm(embeds)\n",
        "\n",
        "        # print(len(hidden[0]))\n",
        "\n",
        "        hidden = hidden[0].squeeze(0)\n",
        "\n",
        "        out = self.fc(hidden)\n",
        "        \n",
        "        output = self.sig(out)\n",
        "      \n",
        "        \n",
        "        ### END YOUR CODE ###\n",
        "        return output\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        hidden = (weight.new(2, batch_size, self.hidden_dim).zero_(),\n",
        "                   weight.new(2, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:56:43.922337Z",
          "iopub.status.busy": "2022-01-22T15:56:43.921742Z",
          "iopub.status.idle": "2022-01-22T15:56:51.579010Z",
          "shell.execute_reply": "2022-01-22T15:56:51.578073Z",
          "shell.execute_reply.started": "2022-01-22T15:56:43.922287Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJflUaNMP0dh",
        "outputId": "0431b958-3834-49e2-cf10-f10f4fe9423a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(RANDOM_SEED)\n",
        "model = RNN(input_dim=len(TEXT.vocab), ### ADD YOUR INPUT DIM HERE. This can be the length of your vocabulary or the embedding dim ###\n",
        "            embedding_dim=50, ### ADD YOUR EMBEDDING DIM HERE ###\n",
        "            hidden_dim=256, ### ADD YOUR HIDDEN DIM HERE ###\n",
        "            output_dim=1  ### ADD NUMBER OF CLASSES HERE ###\n",
        ")\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTEorSIgP0di"
      },
      "source": [
        "# Define Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:56:51.581205Z",
          "iopub.status.busy": "2022-01-22T15:56:51.580712Z",
          "iopub.status.idle": "2022-01-22T15:56:51.589897Z",
          "shell.execute_reply": "2022-01-22T15:56:51.588931Z",
          "shell.execute_reply.started": "2022-01-22T15:56:51.581131Z"
        },
        "trusted": true,
        "id": "KGmYSnnBP0di"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(model, data_loader, device):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        correct_pred, num_examples = 0, 0\n",
        "\n",
        "        for i, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "            targets = targets.float().to(device)\n",
        "\n",
        "            logits = model(features)\n",
        "            # print(logits)\n",
        "            _, predicted_labels = torch.max(logits,1)\n",
        "            # print(predicted_labels)\n",
        "\n",
        "            num_examples += targets.size(0)\n",
        "            correct_pred += (predicted_labels == targets).sum()\n",
        "    return correct_pred.float()/num_examples * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfFzYQWqP0di"
      },
      "source": [
        "# Model Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T15:56:51.596066Z",
          "iopub.status.busy": "2022-01-22T15:56:51.595269Z",
          "iopub.status.idle": "2022-01-22T16:41:22.979843Z",
          "shell.execute_reply": "2022-01-22T16:41:22.979001Z",
          "shell.execute_reply.started": "2022-01-22T15:56:51.596024Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XbMaBYOP0dj",
        "outputId": "75d15091-7de5-4b52-849d-4fae52b7a95a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/030 | Batch 000/680 | Loss: 0.6799\n",
            "Epoch: 001/030 | Batch 050/680 | Loss: 0.6851\n",
            "Epoch: 001/030 | Batch 100/680 | Loss: 0.6611\n",
            "Epoch: 001/030 | Batch 150/680 | Loss: 0.7979\n",
            "Epoch: 001/030 | Batch 200/680 | Loss: 0.5880\n",
            "Epoch: 001/030 | Batch 250/680 | Loss: 0.5773\n",
            "Epoch: 001/030 | Batch 300/680 | Loss: 0.5294\n",
            "Epoch: 001/030 | Batch 350/680 | Loss: 0.5873\n",
            "Epoch: 001/030 | Batch 400/680 | Loss: 0.5678\n",
            "Epoch: 001/030 | Batch 450/680 | Loss: 0.6744\n",
            "Epoch: 001/030 | Batch 500/680 | Loss: 0.4251\n",
            "Epoch: 001/030 | Batch 550/680 | Loss: 0.3697\n",
            "Epoch: 001/030 | Batch 600/680 | Loss: 0.6723\n",
            "Epoch: 001/030 | Batch 650/680 | Loss: 0.4342\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 1.07 min\n",
            "Epoch: 002/030 | Batch 000/680 | Loss: 0.5500\n",
            "Epoch: 002/030 | Batch 050/680 | Loss: 0.4689\n",
            "Epoch: 002/030 | Batch 100/680 | Loss: 0.3067\n",
            "Epoch: 002/030 | Batch 150/680 | Loss: 0.3346\n",
            "Epoch: 002/030 | Batch 200/680 | Loss: 0.4321\n",
            "Epoch: 002/030 | Batch 250/680 | Loss: 0.4983\n",
            "Epoch: 002/030 | Batch 300/680 | Loss: 0.3835\n",
            "Epoch: 002/030 | Batch 350/680 | Loss: 0.4186\n",
            "Epoch: 002/030 | Batch 400/680 | Loss: 0.3273\n",
            "Epoch: 002/030 | Batch 450/680 | Loss: 0.3955\n",
            "Epoch: 002/030 | Batch 500/680 | Loss: 0.4157\n",
            "Epoch: 002/030 | Batch 550/680 | Loss: 0.5276\n",
            "Epoch: 002/030 | Batch 600/680 | Loss: 0.3091\n",
            "Epoch: 002/030 | Batch 650/680 | Loss: 0.5104\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 2.13 min\n",
            "Epoch: 003/030 | Batch 000/680 | Loss: 0.3492\n",
            "Epoch: 003/030 | Batch 050/680 | Loss: 0.1504\n",
            "Epoch: 003/030 | Batch 100/680 | Loss: 0.3306\n",
            "Epoch: 003/030 | Batch 150/680 | Loss: 0.2409\n",
            "Epoch: 003/030 | Batch 200/680 | Loss: 0.2725\n",
            "Epoch: 003/030 | Batch 250/680 | Loss: 0.2091\n",
            "Epoch: 003/030 | Batch 300/680 | Loss: 0.3391\n",
            "Epoch: 003/030 | Batch 350/680 | Loss: 0.3238\n",
            "Epoch: 003/030 | Batch 400/680 | Loss: 0.2803\n",
            "Epoch: 003/030 | Batch 450/680 | Loss: 0.2373\n",
            "Epoch: 003/030 | Batch 500/680 | Loss: 0.2416\n",
            "Epoch: 003/030 | Batch 550/680 | Loss: 0.2431\n",
            "Epoch: 003/030 | Batch 600/680 | Loss: 0.2387\n",
            "Epoch: 003/030 | Batch 650/680 | Loss: 0.4517\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 3.19 min\n",
            "Epoch: 004/030 | Batch 000/680 | Loss: 0.3215\n",
            "Epoch: 004/030 | Batch 050/680 | Loss: 0.1392\n",
            "Epoch: 004/030 | Batch 100/680 | Loss: 0.4234\n",
            "Epoch: 004/030 | Batch 150/680 | Loss: 0.2645\n",
            "Epoch: 004/030 | Batch 200/680 | Loss: 0.1336\n",
            "Epoch: 004/030 | Batch 250/680 | Loss: 0.1044\n",
            "Epoch: 004/030 | Batch 300/680 | Loss: 0.1287\n",
            "Epoch: 004/030 | Batch 350/680 | Loss: 0.3160\n",
            "Epoch: 004/030 | Batch 400/680 | Loss: 0.3460\n",
            "Epoch: 004/030 | Batch 450/680 | Loss: 0.2805\n",
            "Epoch: 004/030 | Batch 500/680 | Loss: 0.1451\n",
            "Epoch: 004/030 | Batch 550/680 | Loss: 0.3120\n",
            "Epoch: 004/030 | Batch 600/680 | Loss: 0.4536\n",
            "Epoch: 004/030 | Batch 650/680 | Loss: 0.2619\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 4.25 min\n",
            "Epoch: 005/030 | Batch 000/680 | Loss: 0.2146\n",
            "Epoch: 005/030 | Batch 050/680 | Loss: 0.1188\n",
            "Epoch: 005/030 | Batch 100/680 | Loss: 0.1982\n",
            "Epoch: 005/030 | Batch 150/680 | Loss: 0.1141\n",
            "Epoch: 005/030 | Batch 200/680 | Loss: 0.1934\n",
            "Epoch: 005/030 | Batch 250/680 | Loss: 0.1502\n",
            "Epoch: 005/030 | Batch 300/680 | Loss: 0.2339\n",
            "Epoch: 005/030 | Batch 350/680 | Loss: 0.2414\n",
            "Epoch: 005/030 | Batch 400/680 | Loss: 0.2729\n",
            "Epoch: 005/030 | Batch 450/680 | Loss: 0.1097\n",
            "Epoch: 005/030 | Batch 500/680 | Loss: 0.3328\n",
            "Epoch: 005/030 | Batch 550/680 | Loss: 0.3071\n",
            "Epoch: 005/030 | Batch 600/680 | Loss: 0.2676\n",
            "Epoch: 005/030 | Batch 650/680 | Loss: 0.3676\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 5.31 min\n",
            "Epoch: 006/030 | Batch 000/680 | Loss: 0.1987\n",
            "Epoch: 006/030 | Batch 050/680 | Loss: 0.2796\n",
            "Epoch: 006/030 | Batch 100/680 | Loss: 0.2460\n",
            "Epoch: 006/030 | Batch 150/680 | Loss: 0.1917\n",
            "Epoch: 006/030 | Batch 200/680 | Loss: 0.2124\n",
            "Epoch: 006/030 | Batch 250/680 | Loss: 0.2070\n",
            "Epoch: 006/030 | Batch 300/680 | Loss: 0.2556\n",
            "Epoch: 006/030 | Batch 350/680 | Loss: 0.1002\n",
            "Epoch: 006/030 | Batch 400/680 | Loss: 0.2858\n",
            "Epoch: 006/030 | Batch 450/680 | Loss: 0.1190\n",
            "Epoch: 006/030 | Batch 500/680 | Loss: 0.1262\n",
            "Epoch: 006/030 | Batch 550/680 | Loss: 0.2805\n",
            "Epoch: 006/030 | Batch 600/680 | Loss: 0.1317\n",
            "Epoch: 006/030 | Batch 650/680 | Loss: 0.3412\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 6.37 min\n",
            "Epoch: 007/030 | Batch 000/680 | Loss: 0.0210\n",
            "Epoch: 007/030 | Batch 050/680 | Loss: 0.1099\n",
            "Epoch: 007/030 | Batch 100/680 | Loss: 0.1310\n",
            "Epoch: 007/030 | Batch 150/680 | Loss: 0.0970\n",
            "Epoch: 007/030 | Batch 200/680 | Loss: 0.1179\n",
            "Epoch: 007/030 | Batch 250/680 | Loss: 0.1441\n",
            "Epoch: 007/030 | Batch 300/680 | Loss: 0.0888\n",
            "Epoch: 007/030 | Batch 350/680 | Loss: 0.1548\n",
            "Epoch: 007/030 | Batch 400/680 | Loss: 0.0458\n",
            "Epoch: 007/030 | Batch 450/680 | Loss: 0.0654\n",
            "Epoch: 007/030 | Batch 500/680 | Loss: 0.1152\n",
            "Epoch: 007/030 | Batch 550/680 | Loss: 0.2184\n",
            "Epoch: 007/030 | Batch 600/680 | Loss: 0.1402\n",
            "Epoch: 007/030 | Batch 650/680 | Loss: 0.0488\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 7.44 min\n",
            "Epoch: 008/030 | Batch 000/680 | Loss: 0.1177\n",
            "Epoch: 008/030 | Batch 050/680 | Loss: 0.0429\n",
            "Epoch: 008/030 | Batch 100/680 | Loss: 0.0260\n",
            "Epoch: 008/030 | Batch 150/680 | Loss: 0.1022\n",
            "Epoch: 008/030 | Batch 200/680 | Loss: 0.0894\n",
            "Epoch: 008/030 | Batch 250/680 | Loss: 0.1430\n",
            "Epoch: 008/030 | Batch 300/680 | Loss: 0.1479\n",
            "Epoch: 008/030 | Batch 350/680 | Loss: 0.0808\n",
            "Epoch: 008/030 | Batch 400/680 | Loss: 0.1381\n",
            "Epoch: 008/030 | Batch 450/680 | Loss: 0.0671\n",
            "Epoch: 008/030 | Batch 500/680 | Loss: 0.0716\n",
            "Epoch: 008/030 | Batch 550/680 | Loss: 0.1342\n",
            "Epoch: 008/030 | Batch 600/680 | Loss: 0.2452\n",
            "Epoch: 008/030 | Batch 650/680 | Loss: 0.1545\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 8.51 min\n",
            "Epoch: 009/030 | Batch 000/680 | Loss: 0.0392\n",
            "Epoch: 009/030 | Batch 050/680 | Loss: 0.0176\n",
            "Epoch: 009/030 | Batch 100/680 | Loss: 0.1069\n",
            "Epoch: 009/030 | Batch 150/680 | Loss: 0.0381\n",
            "Epoch: 009/030 | Batch 200/680 | Loss: 0.1287\n",
            "Epoch: 009/030 | Batch 250/680 | Loss: 0.0333\n",
            "Epoch: 009/030 | Batch 300/680 | Loss: 0.0277\n",
            "Epoch: 009/030 | Batch 350/680 | Loss: 0.0831\n",
            "Epoch: 009/030 | Batch 400/680 | Loss: 0.0645\n",
            "Epoch: 009/030 | Batch 450/680 | Loss: 0.1848\n",
            "Epoch: 009/030 | Batch 500/680 | Loss: 0.3666\n",
            "Epoch: 009/030 | Batch 550/680 | Loss: 0.1516\n",
            "Epoch: 009/030 | Batch 600/680 | Loss: 0.0650\n",
            "Epoch: 009/030 | Batch 650/680 | Loss: 0.0362\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 9.57 min\n",
            "Epoch: 010/030 | Batch 000/680 | Loss: 0.1062\n",
            "Epoch: 010/030 | Batch 050/680 | Loss: 0.1339\n",
            "Epoch: 010/030 | Batch 100/680 | Loss: 0.1072\n",
            "Epoch: 010/030 | Batch 150/680 | Loss: 0.0788\n",
            "Epoch: 010/030 | Batch 200/680 | Loss: 0.2739\n",
            "Epoch: 010/030 | Batch 250/680 | Loss: 0.0460\n",
            "Epoch: 010/030 | Batch 300/680 | Loss: 0.0195\n",
            "Epoch: 010/030 | Batch 350/680 | Loss: 0.0554\n",
            "Epoch: 010/030 | Batch 400/680 | Loss: 0.0737\n",
            "Epoch: 010/030 | Batch 450/680 | Loss: 0.0604\n",
            "Epoch: 010/030 | Batch 500/680 | Loss: 0.1126\n",
            "Epoch: 010/030 | Batch 550/680 | Loss: 0.0292\n",
            "Epoch: 010/030 | Batch 600/680 | Loss: 0.0660\n",
            "Epoch: 010/030 | Batch 650/680 | Loss: 0.0401\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 10.63 min\n",
            "Epoch: 011/030 | Batch 000/680 | Loss: 0.1843\n",
            "Epoch: 011/030 | Batch 050/680 | Loss: 0.0555\n",
            "Epoch: 011/030 | Batch 100/680 | Loss: 0.0348\n",
            "Epoch: 011/030 | Batch 150/680 | Loss: 0.0347\n",
            "Epoch: 011/030 | Batch 200/680 | Loss: 0.0079\n",
            "Epoch: 011/030 | Batch 250/680 | Loss: 0.0346\n",
            "Epoch: 011/030 | Batch 300/680 | Loss: 0.1630\n",
            "Epoch: 011/030 | Batch 350/680 | Loss: 0.0219\n",
            "Epoch: 011/030 | Batch 400/680 | Loss: 0.0540\n",
            "Epoch: 011/030 | Batch 450/680 | Loss: 0.1543\n",
            "Epoch: 011/030 | Batch 500/680 | Loss: 0.0100\n",
            "Epoch: 011/030 | Batch 550/680 | Loss: 0.0425\n",
            "Epoch: 011/030 | Batch 600/680 | Loss: 0.0729\n",
            "Epoch: 011/030 | Batch 650/680 | Loss: 0.0814\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 11.69 min\n",
            "Epoch: 012/030 | Batch 000/680 | Loss: 0.0685\n",
            "Epoch: 012/030 | Batch 050/680 | Loss: 0.0272\n",
            "Epoch: 012/030 | Batch 100/680 | Loss: 0.0131\n",
            "Epoch: 012/030 | Batch 150/680 | Loss: 0.1487\n",
            "Epoch: 012/030 | Batch 200/680 | Loss: 0.0247\n",
            "Epoch: 012/030 | Batch 250/680 | Loss: 0.0617\n",
            "Epoch: 012/030 | Batch 300/680 | Loss: 0.0136\n",
            "Epoch: 012/030 | Batch 350/680 | Loss: 0.0306\n",
            "Epoch: 012/030 | Batch 400/680 | Loss: 0.0216\n",
            "Epoch: 012/030 | Batch 450/680 | Loss: 0.0720\n",
            "Epoch: 012/030 | Batch 500/680 | Loss: 0.0203\n",
            "Epoch: 012/030 | Batch 550/680 | Loss: 0.1903\n",
            "Epoch: 012/030 | Batch 600/680 | Loss: 0.0399\n",
            "Epoch: 012/030 | Batch 650/680 | Loss: 0.0841\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 12.75 min\n",
            "Epoch: 013/030 | Batch 000/680 | Loss: 0.1422\n",
            "Epoch: 013/030 | Batch 050/680 | Loss: 0.0954\n",
            "Epoch: 013/030 | Batch 100/680 | Loss: 0.0674\n",
            "Epoch: 013/030 | Batch 150/680 | Loss: 0.1752\n",
            "Epoch: 013/030 | Batch 200/680 | Loss: 0.0976\n",
            "Epoch: 013/030 | Batch 250/680 | Loss: 0.0107\n",
            "Epoch: 013/030 | Batch 300/680 | Loss: 0.1581\n",
            "Epoch: 013/030 | Batch 350/680 | Loss: 0.1734\n",
            "Epoch: 013/030 | Batch 400/680 | Loss: 0.0266\n",
            "Epoch: 013/030 | Batch 450/680 | Loss: 0.1941\n",
            "Epoch: 013/030 | Batch 500/680 | Loss: 0.0539\n",
            "Epoch: 013/030 | Batch 550/680 | Loss: 0.0327\n",
            "Epoch: 013/030 | Batch 600/680 | Loss: 0.0576\n",
            "Epoch: 013/030 | Batch 650/680 | Loss: 0.1390\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 13.81 min\n",
            "Epoch: 014/030 | Batch 000/680 | Loss: 0.0605\n",
            "Epoch: 014/030 | Batch 050/680 | Loss: 0.0593\n",
            "Epoch: 014/030 | Batch 100/680 | Loss: 0.0607\n",
            "Epoch: 014/030 | Batch 150/680 | Loss: 0.0143\n",
            "Epoch: 014/030 | Batch 200/680 | Loss: 0.0791\n",
            "Epoch: 014/030 | Batch 250/680 | Loss: 0.0465\n",
            "Epoch: 014/030 | Batch 300/680 | Loss: 0.0218\n",
            "Epoch: 014/030 | Batch 350/680 | Loss: 0.1155\n",
            "Epoch: 014/030 | Batch 400/680 | Loss: 0.1075\n",
            "Epoch: 014/030 | Batch 450/680 | Loss: 0.0133\n",
            "Epoch: 014/030 | Batch 500/680 | Loss: 0.0508\n",
            "Epoch: 014/030 | Batch 550/680 | Loss: 0.0539\n",
            "Epoch: 014/030 | Batch 600/680 | Loss: 0.0667\n",
            "Epoch: 014/030 | Batch 650/680 | Loss: 0.0903\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 14.87 min\n",
            "Epoch: 015/030 | Batch 000/680 | Loss: 0.0378\n",
            "Epoch: 015/030 | Batch 050/680 | Loss: 0.0767\n",
            "Epoch: 015/030 | Batch 100/680 | Loss: 0.0592\n",
            "Epoch: 015/030 | Batch 150/680 | Loss: 0.1593\n",
            "Epoch: 015/030 | Batch 200/680 | Loss: 0.0065\n",
            "Epoch: 015/030 | Batch 250/680 | Loss: 0.0127\n",
            "Epoch: 015/030 | Batch 300/680 | Loss: 0.0037\n",
            "Epoch: 015/030 | Batch 350/680 | Loss: 0.0221\n",
            "Epoch: 015/030 | Batch 400/680 | Loss: 0.2810\n",
            "Epoch: 015/030 | Batch 450/680 | Loss: 0.0406\n",
            "Epoch: 015/030 | Batch 500/680 | Loss: 0.0801\n",
            "Epoch: 015/030 | Batch 550/680 | Loss: 0.0573\n",
            "Epoch: 015/030 | Batch 600/680 | Loss: 0.1542\n",
            "Epoch: 015/030 | Batch 650/680 | Loss: 0.0897\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 15.94 min\n",
            "Epoch: 016/030 | Batch 000/680 | Loss: 0.0107\n",
            "Epoch: 016/030 | Batch 050/680 | Loss: 0.0716\n",
            "Epoch: 016/030 | Batch 100/680 | Loss: 0.0421\n",
            "Epoch: 016/030 | Batch 150/680 | Loss: 0.2050\n",
            "Epoch: 016/030 | Batch 200/680 | Loss: 0.0119\n",
            "Epoch: 016/030 | Batch 250/680 | Loss: 0.1362\n",
            "Epoch: 016/030 | Batch 300/680 | Loss: 0.0091\n",
            "Epoch: 016/030 | Batch 350/680 | Loss: 0.0527\n",
            "Epoch: 016/030 | Batch 400/680 | Loss: 0.1136\n",
            "Epoch: 016/030 | Batch 450/680 | Loss: 0.0334\n",
            "Epoch: 016/030 | Batch 500/680 | Loss: 0.0858\n",
            "Epoch: 016/030 | Batch 550/680 | Loss: 0.1507\n",
            "Epoch: 016/030 | Batch 600/680 | Loss: 0.0903\n",
            "Epoch: 016/030 | Batch 650/680 | Loss: 0.0163\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 17.01 min\n",
            "Epoch: 017/030 | Batch 000/680 | Loss: 0.0077\n",
            "Epoch: 017/030 | Batch 050/680 | Loss: 0.0280\n",
            "Epoch: 017/030 | Batch 100/680 | Loss: 0.0850\n",
            "Epoch: 017/030 | Batch 150/680 | Loss: 0.2344\n",
            "Epoch: 017/030 | Batch 200/680 | Loss: 0.0553\n",
            "Epoch: 017/030 | Batch 250/680 | Loss: 0.1261\n",
            "Epoch: 017/030 | Batch 300/680 | Loss: 0.0137\n",
            "Epoch: 017/030 | Batch 350/680 | Loss: 0.0500\n",
            "Epoch: 017/030 | Batch 400/680 | Loss: 0.0573\n",
            "Epoch: 017/030 | Batch 450/680 | Loss: 0.0479\n",
            "Epoch: 017/030 | Batch 500/680 | Loss: 0.0268\n",
            "Epoch: 017/030 | Batch 550/680 | Loss: 0.0172\n",
            "Epoch: 017/030 | Batch 600/680 | Loss: 0.0142\n",
            "Epoch: 017/030 | Batch 650/680 | Loss: 0.2464\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 18.06 min\n",
            "Epoch: 018/030 | Batch 000/680 | Loss: 0.0070\n",
            "Epoch: 018/030 | Batch 050/680 | Loss: 0.0300\n",
            "Epoch: 018/030 | Batch 100/680 | Loss: 0.0313\n",
            "Epoch: 018/030 | Batch 150/680 | Loss: 0.0254\n",
            "Epoch: 018/030 | Batch 200/680 | Loss: 0.0746\n",
            "Epoch: 018/030 | Batch 250/680 | Loss: 0.2701\n",
            "Epoch: 018/030 | Batch 300/680 | Loss: 0.0772\n",
            "Epoch: 018/030 | Batch 350/680 | Loss: 0.0394\n",
            "Epoch: 018/030 | Batch 400/680 | Loss: 0.0146\n",
            "Epoch: 018/030 | Batch 450/680 | Loss: 0.0217\n",
            "Epoch: 018/030 | Batch 500/680 | Loss: 0.0326\n",
            "Epoch: 018/030 | Batch 550/680 | Loss: 0.1339\n",
            "Epoch: 018/030 | Batch 600/680 | Loss: 0.0241\n",
            "Epoch: 018/030 | Batch 650/680 | Loss: 0.0249\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 19.13 min\n",
            "Epoch: 019/030 | Batch 000/680 | Loss: 0.0078\n",
            "Epoch: 019/030 | Batch 050/680 | Loss: 0.0072\n",
            "Epoch: 019/030 | Batch 100/680 | Loss: 0.0257\n",
            "Epoch: 019/030 | Batch 150/680 | Loss: 0.0169\n",
            "Epoch: 019/030 | Batch 200/680 | Loss: 0.0093\n",
            "Epoch: 019/030 | Batch 250/680 | Loss: 0.0361\n",
            "Epoch: 019/030 | Batch 300/680 | Loss: 0.0583\n",
            "Epoch: 019/030 | Batch 350/680 | Loss: 0.0060\n",
            "Epoch: 019/030 | Batch 400/680 | Loss: 0.0091\n",
            "Epoch: 019/030 | Batch 450/680 | Loss: 0.3294\n",
            "Epoch: 019/030 | Batch 500/680 | Loss: 0.0399\n",
            "Epoch: 019/030 | Batch 550/680 | Loss: 0.0081\n",
            "Epoch: 019/030 | Batch 600/680 | Loss: 0.0416\n",
            "Epoch: 019/030 | Batch 650/680 | Loss: 0.0628\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 20.19 min\n",
            "Epoch: 020/030 | Batch 000/680 | Loss: 0.1363\n",
            "Epoch: 020/030 | Batch 050/680 | Loss: 0.0285\n",
            "Epoch: 020/030 | Batch 100/680 | Loss: 0.0106\n",
            "Epoch: 020/030 | Batch 150/680 | Loss: 0.0291\n",
            "Epoch: 020/030 | Batch 200/680 | Loss: 0.0147\n",
            "Epoch: 020/030 | Batch 250/680 | Loss: 0.1510\n",
            "Epoch: 020/030 | Batch 300/680 | Loss: 0.1190\n",
            "Epoch: 020/030 | Batch 350/680 | Loss: 0.0104\n",
            "Epoch: 020/030 | Batch 400/680 | Loss: 0.0038\n",
            "Epoch: 020/030 | Batch 450/680 | Loss: 0.0109\n",
            "Epoch: 020/030 | Batch 500/680 | Loss: 0.1148\n",
            "Epoch: 020/030 | Batch 550/680 | Loss: 0.0571\n",
            "Epoch: 020/030 | Batch 600/680 | Loss: 0.0082\n",
            "Epoch: 020/030 | Batch 650/680 | Loss: 0.0125\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 21.25 min\n",
            "Epoch: 021/030 | Batch 000/680 | Loss: 0.0307\n",
            "Epoch: 021/030 | Batch 050/680 | Loss: 0.0175\n",
            "Epoch: 021/030 | Batch 100/680 | Loss: 0.0151\n",
            "Epoch: 021/030 | Batch 150/680 | Loss: 0.0071\n",
            "Epoch: 021/030 | Batch 200/680 | Loss: 0.0224\n",
            "Epoch: 021/030 | Batch 250/680 | Loss: 0.0080\n",
            "Epoch: 021/030 | Batch 300/680 | Loss: 0.0397\n",
            "Epoch: 021/030 | Batch 350/680 | Loss: 0.0152\n",
            "Epoch: 021/030 | Batch 400/680 | Loss: 0.0037\n",
            "Epoch: 021/030 | Batch 450/680 | Loss: 0.0466\n",
            "Epoch: 021/030 | Batch 500/680 | Loss: 0.0032\n",
            "Epoch: 021/030 | Batch 550/680 | Loss: 0.1184\n",
            "Epoch: 021/030 | Batch 600/680 | Loss: 0.0279\n",
            "Epoch: 021/030 | Batch 650/680 | Loss: 0.0359\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 22.32 min\n",
            "Epoch: 022/030 | Batch 000/680 | Loss: 0.0156\n",
            "Epoch: 022/030 | Batch 050/680 | Loss: 0.2316\n",
            "Epoch: 022/030 | Batch 100/680 | Loss: 0.0080\n",
            "Epoch: 022/030 | Batch 150/680 | Loss: 0.0387\n",
            "Epoch: 022/030 | Batch 200/680 | Loss: 0.0711\n",
            "Epoch: 022/030 | Batch 250/680 | Loss: 0.0116\n",
            "Epoch: 022/030 | Batch 300/680 | Loss: 0.0346\n",
            "Epoch: 022/030 | Batch 350/680 | Loss: 0.0402\n",
            "Epoch: 022/030 | Batch 400/680 | Loss: 0.1167\n",
            "Epoch: 022/030 | Batch 450/680 | Loss: 0.0272\n",
            "Epoch: 022/030 | Batch 500/680 | Loss: 0.1180\n",
            "Epoch: 022/030 | Batch 550/680 | Loss: 0.0175\n",
            "Epoch: 022/030 | Batch 600/680 | Loss: 0.0151\n",
            "Epoch: 022/030 | Batch 650/680 | Loss: 0.2070\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 23.39 min\n",
            "Epoch: 023/030 | Batch 000/680 | Loss: 0.0373\n",
            "Epoch: 023/030 | Batch 050/680 | Loss: 0.0321\n",
            "Epoch: 023/030 | Batch 100/680 | Loss: 0.0597\n",
            "Epoch: 023/030 | Batch 150/680 | Loss: 0.0952\n",
            "Epoch: 023/030 | Batch 200/680 | Loss: 0.0417\n",
            "Epoch: 023/030 | Batch 250/680 | Loss: 0.1084\n",
            "Epoch: 023/030 | Batch 300/680 | Loss: 0.0401\n",
            "Epoch: 023/030 | Batch 350/680 | Loss: 0.0372\n",
            "Epoch: 023/030 | Batch 400/680 | Loss: 0.1105\n",
            "Epoch: 023/030 | Batch 450/680 | Loss: 0.0414\n",
            "Epoch: 023/030 | Batch 500/680 | Loss: 0.0095\n",
            "Epoch: 023/030 | Batch 550/680 | Loss: 0.0056\n",
            "Epoch: 023/030 | Batch 600/680 | Loss: 0.0184\n",
            "Epoch: 023/030 | Batch 650/680 | Loss: 0.1543\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 24.45 min\n",
            "Epoch: 024/030 | Batch 000/680 | Loss: 0.0237\n",
            "Epoch: 024/030 | Batch 050/680 | Loss: 0.0560\n",
            "Epoch: 024/030 | Batch 100/680 | Loss: 0.0453\n",
            "Epoch: 024/030 | Batch 150/680 | Loss: 0.0242\n",
            "Epoch: 024/030 | Batch 200/680 | Loss: 0.1206\n",
            "Epoch: 024/030 | Batch 250/680 | Loss: 0.0123\n",
            "Epoch: 024/030 | Batch 300/680 | Loss: 0.0148\n",
            "Epoch: 024/030 | Batch 350/680 | Loss: 0.0059\n",
            "Epoch: 024/030 | Batch 400/680 | Loss: 0.0496\n",
            "Epoch: 024/030 | Batch 450/680 | Loss: 0.2120\n",
            "Epoch: 024/030 | Batch 500/680 | Loss: 0.0073\n",
            "Epoch: 024/030 | Batch 550/680 | Loss: 0.0741\n",
            "Epoch: 024/030 | Batch 600/680 | Loss: 0.0279\n",
            "Epoch: 024/030 | Batch 650/680 | Loss: 0.0147\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 25.51 min\n",
            "Epoch: 025/030 | Batch 000/680 | Loss: 0.0301\n",
            "Epoch: 025/030 | Batch 050/680 | Loss: 0.0570\n",
            "Epoch: 025/030 | Batch 100/680 | Loss: 0.1190\n",
            "Epoch: 025/030 | Batch 150/680 | Loss: 0.3037\n",
            "Epoch: 025/030 | Batch 200/680 | Loss: 0.0747\n",
            "Epoch: 025/030 | Batch 250/680 | Loss: 0.0306\n",
            "Epoch: 025/030 | Batch 300/680 | Loss: 0.1288\n",
            "Epoch: 025/030 | Batch 350/680 | Loss: 0.1467\n",
            "Epoch: 025/030 | Batch 400/680 | Loss: 0.0047\n",
            "Epoch: 025/030 | Batch 450/680 | Loss: 0.1794\n",
            "Epoch: 025/030 | Batch 500/680 | Loss: 0.1290\n",
            "Epoch: 025/030 | Batch 550/680 | Loss: 0.0665\n",
            "Epoch: 025/030 | Batch 600/680 | Loss: 0.0751\n",
            "Epoch: 025/030 | Batch 650/680 | Loss: 0.1060\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 26.59 min\n",
            "Epoch: 026/030 | Batch 000/680 | Loss: 0.0236\n",
            "Epoch: 026/030 | Batch 050/680 | Loss: 0.1149\n",
            "Epoch: 026/030 | Batch 100/680 | Loss: 0.0316\n",
            "Epoch: 026/030 | Batch 150/680 | Loss: 0.0189\n",
            "Epoch: 026/030 | Batch 200/680 | Loss: 0.0434\n",
            "Epoch: 026/030 | Batch 250/680 | Loss: 0.0923\n",
            "Epoch: 026/030 | Batch 300/680 | Loss: 0.0262\n",
            "Epoch: 026/030 | Batch 350/680 | Loss: 0.0153\n",
            "Epoch: 026/030 | Batch 400/680 | Loss: 0.0108\n",
            "Epoch: 026/030 | Batch 450/680 | Loss: 0.0762\n",
            "Epoch: 026/030 | Batch 500/680 | Loss: 0.0151\n",
            "Epoch: 026/030 | Batch 550/680 | Loss: 0.2637\n",
            "Epoch: 026/030 | Batch 600/680 | Loss: 0.1297\n",
            "Epoch: 026/030 | Batch 650/680 | Loss: 0.0068\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 27.65 min\n",
            "Epoch: 027/030 | Batch 000/680 | Loss: 0.1593\n",
            "Epoch: 027/030 | Batch 050/680 | Loss: 0.0689\n",
            "Epoch: 027/030 | Batch 100/680 | Loss: 0.0801\n",
            "Epoch: 027/030 | Batch 150/680 | Loss: 0.0173\n",
            "Epoch: 027/030 | Batch 200/680 | Loss: 0.1083\n",
            "Epoch: 027/030 | Batch 250/680 | Loss: 0.0169\n",
            "Epoch: 027/030 | Batch 300/680 | Loss: 0.0298\n",
            "Epoch: 027/030 | Batch 350/680 | Loss: 0.0502\n",
            "Epoch: 027/030 | Batch 400/680 | Loss: 0.0962\n",
            "Epoch: 027/030 | Batch 450/680 | Loss: 0.0173\n",
            "Epoch: 027/030 | Batch 500/680 | Loss: 0.1588\n",
            "Epoch: 027/030 | Batch 550/680 | Loss: 0.0078\n",
            "Epoch: 027/030 | Batch 600/680 | Loss: 0.0831\n",
            "Epoch: 027/030 | Batch 650/680 | Loss: 0.0155\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 28.72 min\n",
            "Epoch: 028/030 | Batch 000/680 | Loss: 0.0655\n",
            "Epoch: 028/030 | Batch 050/680 | Loss: 0.0097\n",
            "Epoch: 028/030 | Batch 100/680 | Loss: 0.1695\n",
            "Epoch: 028/030 | Batch 150/680 | Loss: 0.0125\n",
            "Epoch: 028/030 | Batch 200/680 | Loss: 0.0127\n",
            "Epoch: 028/030 | Batch 250/680 | Loss: 0.0034\n",
            "Epoch: 028/030 | Batch 300/680 | Loss: 0.0570\n",
            "Epoch: 028/030 | Batch 350/680 | Loss: 0.0071\n",
            "Epoch: 028/030 | Batch 400/680 | Loss: 0.1057\n",
            "Epoch: 028/030 | Batch 450/680 | Loss: 0.0220\n",
            "Epoch: 028/030 | Batch 500/680 | Loss: 0.0173\n",
            "Epoch: 028/030 | Batch 550/680 | Loss: 0.0173\n",
            "Epoch: 028/030 | Batch 600/680 | Loss: 0.0602\n",
            "Epoch: 028/030 | Batch 650/680 | Loss: 0.1237\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 29.78 min\n",
            "Epoch: 029/030 | Batch 000/680 | Loss: 0.0061\n",
            "Epoch: 029/030 | Batch 050/680 | Loss: 0.0136\n",
            "Epoch: 029/030 | Batch 100/680 | Loss: 0.0096\n",
            "Epoch: 029/030 | Batch 150/680 | Loss: 0.0398\n",
            "Epoch: 029/030 | Batch 200/680 | Loss: 0.0078\n",
            "Epoch: 029/030 | Batch 250/680 | Loss: 0.0049\n",
            "Epoch: 029/030 | Batch 300/680 | Loss: 0.0158\n",
            "Epoch: 029/030 | Batch 350/680 | Loss: 0.0171\n",
            "Epoch: 029/030 | Batch 400/680 | Loss: 0.0814\n",
            "Epoch: 029/030 | Batch 450/680 | Loss: 0.0903\n",
            "Epoch: 029/030 | Batch 500/680 | Loss: 0.0033\n",
            "Epoch: 029/030 | Batch 550/680 | Loss: 0.0365\n",
            "Epoch: 029/030 | Batch 600/680 | Loss: 0.0496\n",
            "Epoch: 029/030 | Batch 650/680 | Loss: 0.0783\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 30.85 min\n",
            "Epoch: 030/030 | Batch 000/680 | Loss: 0.1086\n",
            "Epoch: 030/030 | Batch 050/680 | Loss: 0.0358\n",
            "Epoch: 030/030 | Batch 100/680 | Loss: 0.0200\n",
            "Epoch: 030/030 | Batch 150/680 | Loss: 0.0151\n",
            "Epoch: 030/030 | Batch 200/680 | Loss: 0.0905\n",
            "Epoch: 030/030 | Batch 250/680 | Loss: 0.0074\n",
            "Epoch: 030/030 | Batch 300/680 | Loss: 0.0285\n",
            "Epoch: 030/030 | Batch 350/680 | Loss: 0.0352\n",
            "Epoch: 030/030 | Batch 400/680 | Loss: 0.1392\n",
            "Epoch: 030/030 | Batch 450/680 | Loss: 0.1753\n",
            "Epoch: 030/030 | Batch 500/680 | Loss: 0.0296\n",
            "Epoch: 030/030 | Batch 550/680 | Loss: 0.0132\n",
            "Epoch: 030/030 | Batch 600/680 | Loss: 0.1321\n",
            "Epoch: 030/030 | Batch 650/680 | Loss: 0.0736\n",
            "training accuracy: 50.09%\n",
            "valid accuracy: 50.17%\n",
            "Time elapsed: 31.91 min\n",
            "Total Training Time: 31.91 min\n",
            "Test accuracy: 49.59%\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "# NUM_EPOCHS\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    # h = model.init_hidden(BATCH_SIZE)\n",
        "\n",
        "    for batch_idx, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        text = batch_data.TEXT_COLUMN_NAME.to(DEVICE)\n",
        "        labels = batch_data.LABEL_COLUMN_NAME.to(DEVICE)\n",
        "\n",
        "        ### FORWARD AND BACK PROP\n",
        "        \n",
        "        model.zero_grad()\n",
        "        output = model(text).squeeze(1)\n",
        "        # print(output.shape)\n",
        "        \n",
        "        loss = criterion(output, labels.float())\n",
        "        loss.backward()\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        ### LOGGING\n",
        "        if not batch_idx % 50:\n",
        "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
        "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
        "                   f'Loss: {loss:.4f}')\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        print(f'training accuracy: '\n",
        "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
        "              f'\\nvalid accuracy: '\n",
        "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
        "        \n",
        "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
        "    \n",
        "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
        "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wCIMu9rP0dj"
      },
      "source": [
        "# Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T16:41:22.981494Z",
          "iopub.status.busy": "2022-01-22T16:41:22.981224Z",
          "iopub.status.idle": "2022-01-22T16:41:23.185798Z",
          "shell.execute_reply": "2022-01-22T16:41:23.185126Z",
          "shell.execute_reply.started": "2022-01-22T16:41:22.981459Z"
        },
        "trusted": true,
        "id": "DlyadHKHP0dj",
        "outputId": "a9e10908-fdc7-4967-c587-8a9261242044",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability positive:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.nn.functional.softmax(model(tensor), dim=1)\n",
        "    return prediction[0].item()\n",
        "\n",
        "print('Probability positive:')\n",
        "predict_sentiment(model, \"This is such an awesome movie, I really love it!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-22T16:41:49.753493Z",
          "iopub.status.busy": "2022-01-22T16:41:49.752937Z",
          "iopub.status.idle": "2022-01-22T16:41:49.762087Z",
          "shell.execute_reply": "2022-01-22T16:41:49.761053Z",
          "shell.execute_reply.started": "2022-01-22T16:41:49.753453Z"
        },
        "trusted": true,
        "id": "FkYi2XplP0dk",
        "outputId": "036cacee-b92b-40a7-beb8-f45c772960b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability positive:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "print('Probability positive:')\n",
        "predict_sentiment(model, \"I really hate this movie. It is really bad and sucks!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyY4We1yP0dk"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "HW6-Copy_of_coding_assignment-Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}